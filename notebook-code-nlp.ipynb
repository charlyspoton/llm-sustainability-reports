{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.- Connectors and libraries"
      ],
      "metadata": {
        "id": "0E3Zf9XbXCsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.- Libraries"
      ],
      "metadata": {
        "id": "K41oY_IVXGgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZc8sANgXAow"
      },
      "outputs": [],
      "source": [
        "pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tika"
      ],
      "metadata": {
        "id": "WOOUd1WOXNg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tqdm"
      ],
      "metadata": {
        "id": "bEVOlMlMXPWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Importa tqdm\n",
        "#Standard\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#NLP\n",
        "import fitz\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from tika import parser\n",
        "#Transformers\n",
        "#Sentiment analysis\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from transformers import pipeline  # Hugging Face\n",
        "# pd.set_option(\"display.max_colwidth\", None)\n",
        "#version with Alf updated\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import fitz"
      ],
      "metadata": {
        "id": "AC9ClKrgXQ_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.- Connectors"
      ],
      "metadata": {
        "id": "5JKO1W4SXJDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qfwVMID0XUW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install gspread google-auth\n",
        "\n",
        "# Import libraries\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.auth import default\n",
        "\n",
        "# Authenticate and create a client\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "I5qzASORXXgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read/write from Gsheets\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "try:\n",
        "  import gspread\n",
        "except ModuleNotFoundError:\n",
        "  if 'google.colab' in str(get_ipython()):\n",
        "    %pip install gspread\n",
        "  import gspread\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "def read_gsheet(spreadsheet_id, sheet_name, first_row_as_column_names=True):\n",
        "  sht1 = gc.open_by_key(spreadsheet_id)\n",
        "  worksheet = sht1.worksheet(sheet_name)\n",
        "  df = pd.DataFrame(worksheet.get_all_values())\n",
        "  if first_row_as_column_names == True:\n",
        "    new_header = df.iloc[0]  # Take the first row as the header\n",
        "    df = df[1:]  # Remove the first row from the dataframe\n",
        "    df.columns = new_header\n",
        "  return df\n",
        "def write_gsheet(spreadsheet_id, sheet_name, df, range=None, clean=True):\n",
        "  sht1 = gc.open_by_key(spreadsheet_id)\n",
        "  try:\n",
        "    worksheet = sht1.worksheet(sheet_name)\n",
        "  except BaseException:\n",
        "    worksheet = sht1.add_worksheet(sheet_name,\"999\",\"20\")\n",
        "  if clean:\n",
        "    worksheet.clear()\n",
        "  if range == None:\n",
        "    # worksheet.update([df.columns.tolist()]+ df.astype(str).values.tolist())\n",
        "    worksheet.update([df.columns.tolist()]+ df.fillna('').values.tolist())\n",
        "  elif range != None:\n",
        "    # worksheet.update(range, [df.columns.tolist()]+ df.astype(str).values.tolist())\n",
        "    worksheet.update(range, [df.columns.tolist()]+ df.fillna('').values.tolist())\n",
        "\n",
        "########################################################################################################################################################################\n",
        "\n",
        "# spreadsheet_id = ''\n",
        "# sheet_name = ''\n",
        "# df_read = read_gsheet(\n",
        "#     spreadsheet_id = spreadsheet_id,\n",
        "#     sheet_name = sheet_name,\n",
        "#     first_row_as_column_names = True)\n",
        "\n",
        "\n",
        "# spreadsheet_id = ''\n",
        "# sheet_name = ''\n",
        "# write_gsheet(\n",
        "#     spreadsheet_id = spreadsheet_id,\n",
        "#     sheet_name = sheet_name,\n",
        "#     df = df,\n",
        "#     range = f'A1',\n",
        "#     clean = False)\n",
        "\n",
        "##########################################################################################################################"
      ],
      "metadata": {
        "id": "Qn01-dUUXY70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Functions"
      ],
      "metadata": {
        "id": "1TVoFu5mXboM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.- PDF reading and handling"
      ],
      "metadata": {
        "id": "r_EnYx-MXe-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading PDF from URL: `read_PDF()`"
      ],
      "metadata": {
        "id": "xu26ZSF-bAN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(URL):\n",
        "  res = requests.get(URL)\n",
        "  doc = fitz.open(stream = res.content, filetype=\"pdf\")\n",
        "  return doc"
      ],
      "metadata": {
        "id": "J7TvqYRzbFlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting pages in sentences: `to_list()`\n",
        "\n",
        "The topic analysis (ZSL: Zero-shot-Learning) requires a list as an input"
      ],
      "metadata": {
        "id": "TYbywrDLYpIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_list(page):\n",
        "    text_extracted = page.get_text()\n",
        "    text_normalized = re.sub(r'\\s+', ' ', text_extracted)\n",
        "    sentences = sent_tokenize(text_normalized)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "nuJfP86oXwZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting pages in sentences: `to_dict()`\n",
        "The sentiment analysis model requires a dictionary as an input"
      ],
      "metadata": {
        "id": "lFjGJfkNY3Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dict(page):\n",
        "    text_extracted = page.get_text()\n",
        "    text_normalized = re.sub(r'\\s+', ' ', text_extracted)\n",
        "    sentences = sent_tokenize(text_normalized)\n",
        "    dictionary = {\n",
        "        \"sentences\": sentences\n",
        "    }\n",
        "    return dictionary"
      ],
      "metadata": {
        "id": "AjftteWsZBNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.- Sentiment analysis"
      ],
      "metadata": {
        "id": "9D5c39wTXwxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis functions\n",
        "\n",
        "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Move the model to GPU\n",
        "model.to('cuda')\n",
        "\n",
        "def sentiment_analysis(sentence):\n",
        "    tokenized = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move the input tensors to GPU\n",
        "    tokenized = {k: v.to('cuda') for k, v in tokenized.items()}\n",
        "    # Perform inference on GPU\n",
        "    prediction = model(**tokenized).logits.argmax().item()\n",
        "    return model.config.id2label[prediction]\n",
        "\n",
        "def aggregated_score(analysed):\n",
        "    list_labels = analysed[\"labels\"]\n",
        "    total = len(list_labels)\n",
        "    positive = list_labels.count('POSITIVE')\n",
        "    negative = list_labels.count('NEGATIVE')\n",
        "    score = round((positive / total if total > 0 else 0), 2)\n",
        "    return score"
      ],
      "metadata": {
        "id": "G5jv_cr4Xz37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.- Topic analysis"
      ],
      "metadata": {
        "id": "AX6uXz7WX0NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic classification functions\n",
        "\n",
        "class ZeroShotClassifier:\n",
        "\n",
        "    def create_zsl_model(self, model_name):\n",
        "        \"\"\" Create the zero-shot learning model. \"\"\"\n",
        "        self.model = pipeline(\"zero-shot-classification\", model=model_name, device=0)\n",
        "\n",
        "    def classify_text(self, text, categories):\n",
        "        \"\"\"\n",
        "        Classify text(s) to the pre-defined categories using a\n",
        "        zero-shot classification model and return the raw results.\n",
        "        \"\"\"\n",
        "        # Classify text using the zero-shot transformers model\n",
        "        hypothesis_template = \"This text is about {}.\"\n",
        "        result = self.model(text, categories, multi_label=True,\n",
        "                            hypothesis_template=hypothesis_template)\n",
        "        return result\n",
        "\n",
        "    def text_labels(self, text, category_dict, cutoff=None):\n",
        "        \"\"\"\n",
        "        Classify a text into the pre-defined categories. If cutoff\n",
        "        is defined, return only those entries where the score > cutoff\n",
        "        \"\"\"\n",
        "        # Run the model on our categories\n",
        "        categories = list(category_dict.keys())\n",
        "        result = self.classify_text(text, categories)\n",
        "\n",
        "        # Format as a pandas dataframe and add ESG label\n",
        "        df = pd.DataFrame(result).explode([\"labels\", \"scores\"])\n",
        "        df[\"ESG\"] = df.labels.map(category_dict)\n",
        "\n",
        "        # If a cutoff is provided, filter the dataframe\n",
        "        if cutoff:\n",
        "            df = df[df.scores.gt(cutoff)].copy()\n",
        "        return df.reset_index(drop=True)\n",
        "\n",
        "zs_categories = {\n",
        "  \"sustainability\": \"S\",\n",
        "  \"finance\": \"F\"\n",
        "}\n",
        "\n",
        "# Define and Create the zero-shot learning model\n",
        "model_zs_name = \"valhalla/distilbart-mnli-12-3\"\n",
        "# a smaller version: \"microsoft/deberta-base-mnli\"\n",
        "ZSC = ZeroShotClassifier()\n",
        "ZSC.create_zsl_model(model_zs_name)"
      ],
      "metadata": {
        "id": "lE5b00rLX3Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.- Scoring function: `from_doc_to_combined_scores`"
      ],
      "metadata": {
        "id": "Acxra4bTYBaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def from_doc_to_combined_scores(URL):\n",
        "    # Extraemos la información en páginas\n",
        "    doc = read_pdf(URL)\n",
        "    data_score = []\n",
        "\n",
        "    for page in tqdm(doc, desc=\"Processing pages\"):  # Añade tqdm aquí para mostrar la barra de progreso\n",
        "        # Convertimos cada página en oraciones en un diccionario (limpio)\n",
        "        sentences = to_list(page)\n",
        "\n",
        "        if not sentences:\n",
        "            # Si no hay oraciones en la página, agrega un 0 para cada puntaje\n",
        "            data_score.append({\n",
        "                'Page': page.number,\n",
        "                's_score': 0,\n",
        "                'f_score': 0,\n",
        "                'sentiment_score': 0\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Analizamos el sentimiento para cada oración en la página\n",
        "        classified = ZSC.text_labels(sentences, zs_categories)\n",
        "\n",
        "        # Convertimos a float las puntuaciones\n",
        "        classified['scores'] = classified['scores'].astype('float')\n",
        "\n",
        "        # Filtramos los puntajes de sostenibilidad\n",
        "        subset_s = classified[classified.ESG == \"S\"]\n",
        "        page_score_s = subset_s.scores.mean() if not subset_s.empty else 0\n",
        "\n",
        "        # Filtramos los puntajes de finanzas\n",
        "        subset_f = classified[classified.ESG == \"F\"]\n",
        "        page_score_f = subset_f.scores.mean() if not subset_f.empty else 0\n",
        "\n",
        "        # Analizamos el sentimiento para cada oración en la página\n",
        "        dict_sentences = to_dict(page)\n",
        "        dict_sentences[\"labels\"] = [sentiment_analysis(sentence) for sentence in dict_sentences[\"sentences\"]]\n",
        "\n",
        "        # Calculamos el puntaje agregado\n",
        "        aggregated_page_score = aggregated_score(dict_sentences)\n",
        "\n",
        "        data_score.append({\n",
        "            'Page': page.number,\n",
        "            's_score': page_score_s,\n",
        "            'f_score': page_score_f,\n",
        "            'sentiment_score': aggregated_page_score\n",
        "        })\n",
        "\n",
        "    scores = pd.DataFrame(data_score)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "YJyQL1cCYFyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.- Scoring"
      ],
      "metadata": {
        "id": "afbHt9bhX3Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 10"
      ],
      "metadata": {
        "id": "0s-B4zMPX7Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_table = read_gsheet(\n",
        "    spreadsheet_id = '13vP98GvbJZeRjm-23za6BGD74KdJfpBp4PDWzAIhI8Q',\n",
        "    sheet_name = 'input',\n",
        "    first_row_as_column_names = True)\n",
        "for index, row in input_table.iterrows():\n",
        "    #Read the output to check if it is already there\n",
        "    output_completed = read_gsheet(\n",
        "    spreadsheet_id = '13vP98GvbJZeRjm-23za6BGD74KdJfpBp4PDWzAIhI8Q',\n",
        "    sheet_name = 'OUTPUT_completed',\n",
        "    first_row_as_column_names = True)\n",
        "    #We extract the data needed from every row\n",
        "    company_id = row['company_id']\n",
        "    company_name = row['company_name']\n",
        "    fiscal_year = row['fiscal_year']\n",
        "    report_link = row['report_link']\n",
        "    oy = company_name + fiscal_year\n",
        "    #We check if already there\n",
        "    print('Checking', company_name, 'report for', fiscal_year)\n",
        "    if oy in output_completed.OY.unique():\n",
        "      print('   | Skipped because this company-year is already scored')\n",
        "    else:\n",
        "      print('   |Not scored, scoring...')\n",
        "      print('   |',report_link)\n",
        "      raw = from_doc_to_combined_scores(URL=report_link)\n",
        "      df = raw\n",
        "      df['sustainability_score'] = df['s_score'] / (df['s_score'] + df['f_score'])\n",
        "      #To check this because it is nan\n",
        "      norm_sustainability = (df['sustainability_score'] - df['sustainability_score'].min()) / (df['sustainability_score'].max() - df['sustainability_score'].min())\n",
        "      threshold = 100 - threshold\n",
        "      sustainability_threshold = np.percentile(norm_sustainability, threshold)\n",
        "      high_sustainability_df = df[norm_sustainability >= sustainability_threshold]\n",
        "      sentiment_in_sustainability_sections = round(high_sustainability_df.sentiment_score.mean(),3)\n",
        "      raw = df\n",
        "      raw['company_id'] = company_id\n",
        "      raw['company_name'] = company_name\n",
        "      raw['fiscal_year'] = fiscal_year\n",
        "      raw['OY'] = company_name + fiscal_year\n",
        "      #We append raw to the output_completed\n",
        "      output_completed = pd.concat([output_completed,raw],ignore_index=True)\n",
        "      #We write the results updated in Google sheets\n",
        "      write_gsheet(\n",
        "      spreadsheet_id = '13vP98GvbJZeRjm-23za6BGD74KdJfpBp4PDWzAIhI8Q',\n",
        "      sheet_name = 'OUTPUT_completed',\n",
        "      df = output_completed,\n",
        "      range = f'A1',\n",
        "      clean = True)"
      ],
      "metadata": {
        "id": "gKR6MdC2X8li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.- EDA"
      ],
      "metadata": {
        "id": "5tp_244d9fTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "o9dpaFrwCCCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = read_gsheet(\n",
        "    spreadsheet_id = '1VHMLOVUY9heQX1qq9eVdIrIh3ewwMQYypye9EvL_QK4',\n",
        "    sheet_name = 'OUTPUT_Mayo',\n",
        "    first_row_as_column_names = True)"
      ],
      "metadata": {
        "id": "TfOCPJLs9iQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = scores\n",
        "df = df.fillna(0)"
      ],
      "metadata": {
        "id": "bNgCjF8D9m5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate metrics at OY (Organization - Year) level:"
      ],
      "metadata": {
        "id": "qgt6_PfI9yTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Supongamos que tu DataFrame se llama df\n",
        "# Aquí te muestro cómo sería el DataFrame de ejemplo:\n",
        "# df = pd.DataFrame({\n",
        "#     'OY': [...],\n",
        "#     'fiscal_year': [...],\n",
        "#     'Page': [...],\n",
        "#     'sentiment_score': [...],\n",
        "#     'sustainability_score': [...]\n",
        "# })\n",
        "\n",
        "def calculate_metrics(df):\n",
        "    # Seleccionar solo las columnas necesarias\n",
        "    df = df[['OY', 'fiscal_year', 'Page', 'sentiment_score', 'sustainability_score']]\n",
        "\n",
        "    # Convertir sentiment_score y sustainability_score a float, manejando valores no convertibles\n",
        "    df['sentiment_score'] = pd.to_numeric(df['sentiment_score'], errors='coerce')\n",
        "    df['sustainability_score'] = pd.to_numeric(df['sustainability_score'], errors='coerce')\n",
        "\n",
        "    # Eliminar filas con NaN en sentiment_score o sustainability_score\n",
        "    df = df.dropna(subset=['sentiment_score', 'sustainability_score'])\n",
        "\n",
        "    # Group by OY and calculate mean of sentiment_score and sustainability_score\n",
        "    mean_scores = df.groupby('OY').agg({\n",
        "        'sentiment_score': 'mean',\n",
        "        'sustainability_score': 'mean'\n",
        "    }).rename(columns={\n",
        "        'sentiment_score': 'mean_sentiment_score',\n",
        "        'sustainability_score': 'mean_sustainability_score'\n",
        "    })\n",
        "\n",
        "    # Calculate mean sentiment_score for the top 10% pages by sustainability_score for each OY\n",
        "    top_10_sentiment = df.groupby('OY').apply(\n",
        "        lambda x: x.nlargest(int(len(x) * 0.1), 'sustainability_score')['sentiment_score'].mean()\n",
        "    ).reset_index().rename(columns={0: 'top_10_percent_sentiment_score'})\n",
        "\n",
        "    # Merge the results into a single DataFrame\n",
        "    result = mean_scores.merge(top_10_sentiment, on='OY')\n",
        "\n",
        "    return result\n",
        "\n",
        "# Ejemplo de uso\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "result_df = calculate_metrics(df)"
      ],
      "metadata": {
        "id": "fE3zqMRr9xkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add Taxonomy data:"
      ],
      "metadata": {
        "id": "BybxRSyI96oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "taxonomy = read_gsheet(\n",
        "    spreadsheet_id = '1VHMLOVUY9heQX1qq9eVdIrIh3ewwMQYypye9EvL_QK4',\n",
        "    sheet_name = 'INPUT',\n",
        "    first_row_as_column_names = True)"
      ],
      "metadata": {
        "id": "rjzf52rQ936C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxonomy = taxonomy[['industry','company_name','fiscal_year', 'report_type','revenue_alignment','CAPEX_alignment']]"
      ],
      "metadata": {
        "id": "cbUYqswt9_AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_id = df[['company_name','fiscal_year','OY']]\n",
        "names_id = names_id.drop_duplicates()"
      ],
      "metadata": {
        "id": "XoRiQIIjd9Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.merge(names_id, on=['OY'])"
      ],
      "metadata": {
        "id": "zPwSEKGq-Dfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.merge(taxonomy,on=['company_name','fiscal_year'])"
      ],
      "metadata": {
        "id": "PtziTwo8-F9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot:"
      ],
      "metadata": {
        "id": "_lPMBOKO-GkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check correlation between variables: sentiment score and revenue alignment and capex alignment"
      ],
      "metadata": {
        "id": "ZDlqxGOA-L92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_revenue = result_df[['OY', 'mean_sentiment_score','revenue_alignment']]\n",
        "check_revenue['mean_sentiment_score'] = pd.to_numeric(check_revenue['mean_sentiment_score'], errors='coerce')\n",
        "check_revenue['revenue_alignment'] = pd.to_numeric(check_revenue['revenue_alignment'], errors='coerce')\n",
        "check_revenue['revenue_alignment'] = check_revenue['revenue_alignment']/100"
      ],
      "metadata": {
        "id": "li5JM2SR-La4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "i4k_maqi-IQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Establecer el estilo de seaborn\n",
        "sns.set(style=\"ticks\")\n",
        "\n",
        "# Crear el gráfico de dispersión\n",
        "plt.figure(figsize=(8,8))\n",
        "scatter_plot = sns.scatterplot(data=check_revenue, x='revenue_alignment', y='mean_sentiment_score', palette='viridis',\n",
        "                               sizes=(300,300),s=100, alpha=0.6, edgecolor=None)\n",
        "\n",
        "plt.xlabel('Sustainability Performance', fontsize=14)\n",
        "plt.ylabel('Sentiment Analysis', fontsize=14)\n",
        "\n",
        "plt.xlim(0, 1)  # Cambia los valores según tus datos\n",
        "plt.ylim(0, 1)   # Cambia los valores según tus datos\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gcNcwZ2m-Rgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurar que las columnas necesarias son de tipo float\n",
        "result_df['mean_sentiment_score'] = pd.to_numeric(result_df['mean_sentiment_score'], errors='coerce')\n",
        "result_df['revenue_alignment'] = pd.to_numeric(result_df['revenue_alignment'], errors='coerce')\n",
        "\n",
        "# # Dividir revenue_alignment entre 100\n",
        "# result_df['revenue_alignment'] = result_df['revenue_alignment'] / 100\n",
        "\n",
        "# Reestructurar el DataFrame para tener las métricas en la misma columna\n",
        "melted_df = result_df.melt(id_vars='company_name', value_vars=['mean_sentiment_score', 'revenue_alignment'],\n",
        "                           var_name='metric', value_name='value')\n",
        "\n",
        "# Crear el box plot combinado\n",
        "plt.figure(figsize=(14, 4))\n",
        "sns.boxplot(x='company_name', y='value', hue='metric', data=melted_df)\n",
        "plt.title('Mean Sentiment Score and Revenue Alignment by Company')\n",
        "plt.xlabel('Company')\n",
        "plt.ylabel('Value')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Metric')\n",
        "\n",
        "# Añadir líneas discontinuas entre las compañías\n",
        "num_companies = melted_df['company_name'].nunique()\n",
        "for i in range(num_companies - 1):\n",
        "    plt.axvline(x=i + 0.5, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0m2d6in8-Uk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.- EDA para las secciones de sostenibilidad"
      ],
      "metadata": {
        "id": "mAYqsqJ3-_sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_revenue_sust = result_df[['OY', 'revenue_alignment','top_10_percent_sentiment_score']]\n",
        "check_revenue_sust['mean_sentiment_score'] = pd.to_numeric(check_revenue_sust['top_10_percent_sentiment_score'], errors='coerce')\n",
        "check_revenue_sust['revenue_alignment'] = pd.to_numeric(check_revenue_sust['revenue_alignment'], errors='coerce')\n",
        "check_revenue_sust['revenue_alignment'] = check_revenue_sust['revenue_alignment']/100"
      ],
      "metadata": {
        "id": "SZxWTsKN-haY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Establecer el estilo de seaborn\n",
        "sns.set(style=\"ticks\")\n",
        "\n",
        "# Crear el gráfico de dispersión\n",
        "plt.figure(figsize=(8,8))\n",
        "scatter_plot = sns.scatterplot(data=check_revenue_sust, x='revenue_alignment', y='top_10_percent_sentiment_score', palette='viridis',\n",
        "                               sizes=(300,300),s=100, alpha=0.6, edgecolor=None)\n",
        "\n",
        "plt.xlabel('Sustainability Performance', fontsize=14)\n",
        "plt.ylabel('Sentiment Analysis', fontsize=14)\n",
        "\n",
        "plt.xlim(0, 1)  # Cambia los valores según tus datos\n",
        "plt.ylim(0, 1)   # Cambia los valores según tus datos\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y7eNxr5o_ION"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Correlación de Pearson para revenue_alignment y mean_sentiment_score\n",
        "corr_revenue, p_value_revenue = pearsonr(check_revenue_sust['revenue_alignment'], check_revenue_sust['top_10_percent_sentiment_score'])\n",
        "\n",
        "print(f'Pearson correlation (Revenue): {corr_revenue}, p-value: {p_value_revenue}')\n"
      ],
      "metadata": {
        "id": "WzVp1bvo_LkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "# Análisis de regresión para revenue_alignment y mean_sentiment_score\n",
        "X_revenue = sm.add_constant(check_revenue_sust['revenue_alignment'])\n",
        "model_revenue = sm.OLS(check_revenue_sust['top_10_percent_sentiment_score'], X_revenue).fit()\n",
        "\n",
        "print(model_revenue.summary())"
      ],
      "metadata": {
        "id": "cSyvDUQC_Nfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Supongamos que tu DataFrame se llama df\n",
        "# Ejemplo de DataFrame:\n",
        "# df = pd.DataFrame({\n",
        "#     'company_id': [...],\n",
        "#     'company_name': [...],\n",
        "#     'fiscal_year': [...],\n",
        "#     'OY': [...],\n",
        "#     'Page': [...],\n",
        "#     's_score': [...],\n",
        "#     'f_score': [...],\n",
        "#     'sentiment_score': [...],\n",
        "#     'sustainability_score': [...]\n",
        "# })\n",
        "\n",
        "# Convertir 'Page' y 'sentiment_score' a numérico\n",
        "examples['Page'] = pd.to_numeric(examples['Page'], errors='coerce')\n",
        "examples['sustainability_score'] = pd.to_numeric(examples['sustainability_score'], errors='coerce')\n",
        "\n",
        "# Eliminar filas con valores nulos en 'Page' o 'sentiment_score'\n",
        "examples = examples.dropna(subset=['Page', 'sustainability_score'])\n",
        "\n",
        "examples['fiscal_year'] = pd.Categorical(examples['fiscal_year'], categories=sorted(examples['fiscal_year'].unique()), ordered=True)\n",
        "\n",
        "# Función para aplicar el suavizado\n",
        "def smooth(data, window_size=15):\n",
        "    return uniform_filter1d(data, size=window_size)\n",
        "\n",
        "# Extraer colores de la paleta 'viridis'\n",
        "viridis = cm.get_cmap('viridis')\n",
        "original_color = viridis(0.7)  # Color para la línea original\n",
        "smooth_color = viridis(0.2)    # Color para la línea suavizada\n",
        "mean_color = viridis(0)      # Color para la línea de promedio\n",
        "\n",
        "# Ajustar el estilo y tamaño de fuente\n",
        "sns.set_context(\"talk\", font_scale=1.2)\n",
        "\n",
        "# Crear la matriz de gráficos sin normalizar las páginas\n",
        "g = sns.FacetGrid(examples, row='company_name', col='fiscal_year', margin_titles=True, height=7, aspect=1, palette='viridis')\n",
        "\n",
        "# Mapear tanto la línea original como la suavizada\n",
        "g.map(sns.lineplot, 'Page', 'sustainability_score', color=original_color, alpha=0.5)\n",
        "g.map(lambda x, y, **kwargs: plt.plot(x, smooth(y), color=smooth_color, linewidth=2), 'Page', 'sustainability_score')\n",
        "\n",
        "# Función para agregar la línea de promedio\n",
        "def add_mean_line(data, **kwargs):\n",
        "    plt.axhline(y=data['sustainability_score'].mean(), color=mean_color, linestyle='--', linewidth=1)\n",
        "\n",
        "# Añadir la línea de promedio a cada gráfico\n",
        "g.map_dataframe(add_mean_line)\n",
        "\n",
        "# Añadir títulos y ajustar el layout\n",
        "g.set_axis_labels('Page', 'Sustainability topic Score')\n",
        "g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
        "g.fig.subplots_adjust(top=0.95)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bzrakOsZ_Uek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}